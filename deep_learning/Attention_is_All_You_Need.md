# Attention Is All You Need

Whew ... this is one of those seminal papers, and it's dominated the field of
NLP.

The paper proposes the *Transformer* network architecture for seq2seq tasks,
which outperforms RNNs and CNNs (convs=over time, so can be for non-image based
data).  It uses several attention mechanisms and replicated layers (with skip
connections).

Fortunately, Google's implementation is open source so I'll just use that if I
need it. That's probably the easiest way to get up to speed on it.

Update: after reviewing it, I feel like I get the gist of it. Very cool.

http://jalammar.github.io/illustrated-transformer/
http://nlp.seas.harvard.edu/2018/04/03/attention.html
https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html
